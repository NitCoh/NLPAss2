{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Ass2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4UnVXACVsSF/wwYJ1/wps",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NitCoh/NLPAss2/blob/master/NLP_Ass2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9i7dduqdq0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import GaussianNB\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFXFasvIoOwl",
        "colab_type": "code",
        "outputId": "48135a04-39aa-4151-f089-f10e907e40f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "!wget 'https://www.cs.bgu.ac.il/~elhadad/nlp20/hw2_1_data.tar.gz'\n",
        "!tar -xvzf hw2_1_data.tar.gz\n",
        "!mv data ../\n",
        "!wget https://www.cs.bgu.ac.il/~elhadad/nlp20/ngram_counts.txt.gz\n",
        "!mv ngram_counts.txt.gz ../data/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-16 13:04:31--  https://www.cs.bgu.ac.il/~elhadad/nlp20/hw2_1_data.tar.gz\n",
            "Resolving www.cs.bgu.ac.il (www.cs.bgu.ac.il)... 132.72.41.239\n",
            "Connecting to www.cs.bgu.ac.il (www.cs.bgu.ac.il)|132.72.41.239|:443... connected.\n",
            "ERROR: cannot verify www.cs.bgu.ac.il's certificate, issued by ‘CN=Let's Encrypt Authority X3,O=Let's Encrypt,C=US’:\n",
            "  Unable to locally verify the issuer's authority.\n",
            "To connect to www.cs.bgu.ac.il insecurely, use `--no-check-certificate'.\n",
            "tar (child): hw2_1_data.tar.gz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n",
            "mv: cannot stat 'data': No such file or directory\n",
            "--2020-01-16 13:04:42--  https://www.cs.bgu.ac.il/~elhadad/nlp20/ngram_counts.txt.gz\n",
            "Resolving www.cs.bgu.ac.il (www.cs.bgu.ac.il)... 132.72.41.239\n",
            "Connecting to www.cs.bgu.ac.il (www.cs.bgu.ac.il)|132.72.41.239|:443... connected.\n",
            "ERROR: cannot verify www.cs.bgu.ac.il's certificate, issued by ‘CN=Let's Encrypt Authority X3,O=Let's Encrypt,C=US’:\n",
            "  Unable to locally verify the issuer's authority.\n",
            "To connect to www.cs.bgu.ac.il insecurely, use `--no-check-certificate'.\n",
            "mv: cannot stat 'ngram_counts.txt.gz': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUyxvJjQeKLE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "5578a2d6-5d04-4410-b958-ad3a6b97b7e3"
      },
      "source": [
        "#############################################################\n",
        "## ASSIGNMENT 2_1 CODE SKELETON\n",
        "#############################################################\n",
        "\n",
        "from collections import defaultdict\n",
        "import gzip\n",
        "\n",
        "#### Q1.1 Evaluation Metrics ####\n",
        "\n",
        "## Input: y_pred, a list of length n with the predicted labels,\n",
        "## y_true, a list of length n with the true labels\n",
        "\n",
        "data_path = '../data'\n",
        "\n",
        "## Calculates the precision of the predicted labels\n",
        "def get_precision(y_pred, y_true):\n",
        "  # For this assignment, complex words 1 are considered positive examples, and simple words 0 are considered negative examples.\n",
        "    y_pred = [round(y) for y in y_pred]\n",
        "    tp = len([x for x,y in zip(y_pred,y_true) if x==y==1])\n",
        "    fp = len([x for x,y in zip(y_pred,y_true) if y==0 and x==1])\n",
        "    precision = tp/(tp+fp)\n",
        "    return precision\n",
        "    \n",
        "## Calculates the recall of the predicted labels\n",
        "def get_recall(y_pred, y_true):\n",
        "  \n",
        "    ## YOUR CODE HERE...\n",
        "    y_pred = [round(y) for y in y_pred]\n",
        "    tp = len([x for x,y in zip(y_pred,y_true) if x==y==1])\n",
        "    fn = len([x for x,y in zip(y_pred,y_true) if y==1 and x==0])\n",
        "    recall = tp/(tp+fn)\n",
        "    return recall\n",
        "\n",
        "## Calculates the f-score of the predicted labels\n",
        "def get_fscore(y_pred, y_true):\n",
        "    ## YOUR CODE HERE...\n",
        "    p = get_precision(y_pred,y_true)\n",
        "    r = get_recall(y_pred,y_true)\n",
        "    fscore = 2*p*r / (p +r)\n",
        "    return fscore\n",
        "\n",
        "def test_predictions(y_pred,y_true):\n",
        "    p = get_precision(y_pred,y_true)\n",
        "    r = get_recall(y_pred,y_true)\n",
        "    f1 = get_fscore(y_pred,y_true)\n",
        "    print(\"Precision:\",p)\n",
        "    print(\"Recall:\",r)\n",
        "    print(\"F-score:\",f1)\n",
        "    return p, r, f1\n",
        "\n",
        "#### 2. Complex Word Identification ####\n",
        "\n",
        "## Loads in the words and labels of one of the datasets\n",
        "def load_file(data_file):\n",
        "    words = []\n",
        "    labels = []   \n",
        "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
        "        i = 0\n",
        "        for line in f:\n",
        "            if i > 0:\n",
        "                line_split = line[:-1].split(\"\\t\")\n",
        "                words.append(line_split[0].lower())\n",
        "                labels.append(int(line_split[1]))\n",
        "            i += 1\n",
        "    return words, labels\n",
        "\n",
        "### 1.2.1: A very simple baseline\n",
        "\n",
        "## Labels every word complex\n",
        "def all_complex(data_file):\n",
        "    ## YOUR CODE HERE...\n",
        "    _, y_true = load_file(data_file)\n",
        "    y_pred = [1]*len(y_true)\n",
        "    p, r, f1 = test_predictions(y_pred,y_true)\n",
        "\n",
        "    performance = [p, r, f1]\n",
        "    return performance\n",
        "\n",
        "\n",
        "\n",
        "def threshold_plot_helper(training_file,development_file,pred,ts_range):\n",
        "    p = []\n",
        "    r = []\n",
        "    f1= []\n",
        "    train_words,train_labels = load_file(training_file)\n",
        "    for threshold in ts_range:\n",
        "      y_pred_train = [int(pred(x,threshold)) for x in train_words]\n",
        "      p_t, r_t, f1_t = test_predictions(y_pred_train,train_labels)\n",
        "      p.append(p_t)\n",
        "      r.append(r_t)\n",
        "      f1.append(f1_t)\n",
        "    plt.ylabel('Precision')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.plot(r,p)\n",
        "\n",
        "    best_index =  f1.index(max(f1))\n",
        "    best_threshold = ts_range[best_index]\n",
        "    print(\"Best threshold:\",best_threshold)\n",
        "\n",
        "\n",
        "    dev_words,dev_labels = load_file(development_file)\n",
        "    y_pred_dev = [int(pred(x,best_threshold)) for x in dev_words]\n",
        "    dprecision, drecall, dfscore = test_predictions(y_pred_dev,dev_labels)\n",
        "\n",
        "\n",
        "    training_performance = [p[best_index], r[best_index], f1[best_index]]\n",
        "    development_performance = [dprecision, drecall, dfscore]\n",
        "    return training_performance, development_performance\n",
        "\n",
        "### 1.2.2: Word length thresholding\n",
        "\n",
        "## Finds the best length threshold by f-score, and uses this threshold to\n",
        "## classify the training and development set\n",
        "def word_length_threshold(training_file, development_file):\n",
        "\n",
        "    func = lambda x,threshold: len(x) >= threshold\n",
        "    return threshold_plot_helper(training_file,development_file,func,range(5,11))\n",
        "\n",
        "### 1.2.3: Word frequency thresholding\n",
        "\n",
        "## Loads Google NGram counts\n",
        "def load_ngram_counts(ngram_counts_file): \n",
        "   counts = defaultdict(int) \n",
        "   with gzip.open(ngram_counts_file, 'rt', errors='ignore') as f: \n",
        "       for line in f:\n",
        "           token, count = line.strip().split('\\t') \n",
        "           if token[0].islower(): \n",
        "               counts[token] = int(count) \n",
        "   return counts\n",
        "\n",
        "# Finds the best frequency threshold by f-score, and uses this threshold to\n",
        "## classify the training and development set\n",
        "def word_frequency_threshold(training_file, development_file, counts):\n",
        "    func = lambda word,threshold : counts[word] >= threshold\n",
        "\n",
        "    return threshold_plot_helper(training_file,development_file,func,range(100,8000,100))\n",
        "\n",
        "### 1.3.1: Naive Bayes\n",
        "        \n",
        "def dataset_params(words,counts):\n",
        "  mean_length = sum([len(word) for word in words])/len(words)\n",
        "  std_length = sum([len(word) - mean_length for word in words])/len(words)\n",
        "\n",
        "  mean_counts = sum([counts[word] for word in words])/len(words)\n",
        "  std_counts = sum([counts[word] - mean_counts for word in words])/len(words)\n",
        "\n",
        "  return mean_length,std_length,mean_counts,std_counts\n",
        "\n",
        "## Trains a Naive Bayes classifier using length and frequency features\n",
        "def naive_bayes(training_file, development_file, counts):\n",
        "    \n",
        "\n",
        "    def get_features(word,params):\n",
        "      mean_length,std_length,mean_counts,std_counts = params\n",
        "      l = (len(word) - mean_length) / std_length\n",
        "      c = (counts[word] - mean_counts) / std_counts\n",
        "      return [l,c]\n",
        "\n",
        "    def get_features2(word,params):\n",
        "      l = len(word)\n",
        "      c = counts[word]\n",
        "      return [l,c]\n",
        "\n",
        "\n",
        "    train_words,train_labels = load_file(training_file)\n",
        "    params = dataset_params(train_words,counts)\n",
        "    print(\"PARAMS:\",params)\n",
        "    X_train = np.array([np.array(get_features(word,params)) for word in train_words])\n",
        "    y_true_train = np.array(train_labels)\n",
        "    print(\"-----------------------------------\")\n",
        "    print(X_train)\n",
        "    print(\"-----------------------------------\")\n",
        "\n",
        "    clf = GaussianNB()\n",
        "    clf.fit(X_train,y_true_train)\n",
        "    y_pred_train = clf.predict(X_train)\n",
        "    tprecision, trecall, tfscore = test_predictions(y_pred_train,y_true_train)\n",
        "    training_performance = [tprecision, trecall, tfscore]\n",
        "\n",
        "    #dev\n",
        "    dev_words,dev_labels = load_file(development_file)\n",
        "\n",
        "    X_test = np.array([np.array(get_features(word,params)) for word in dev_words])\n",
        "    y_true_test = np.array(dev_labels)\n",
        "\n",
        "    y_pred_test = clf.predict(X_test)\n",
        "  \n",
        "    dprecision, drecall, dfscore = test_predictions(y_pred_test,y_true_test)\n",
        "    development_performance = [dprecision, drecall, dfscore]\n",
        "    return training_performance, development_performance\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    training_file = \"../data/complex_words_training.txt\"\n",
        "    development_file = \"../data/complex_words_development.txt\"\n",
        "    test_file = \"../data/complex_words_test_unlabeled.txt\"\n",
        "    train_data = load_file(training_file)\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-58b271d96c68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mdevelopment_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../data/complex_words_development.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mtest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../data/complex_words_test_unlabeled.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-58b271d96c68>\u001b[0m in \u001b[0;36mload_file\u001b[0;34m(data_file)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/complex_words_training.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1LngW4AWhce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = np.array([np.array([a,b])for a,b in enumerate([1,2,3,4])])\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3xNjASeoMSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file_name = data_path+'/complex_words_training.txt'\n",
        "dev_file_name = data_path+'/complex_words_development.txt'\n",
        "test_file_name = data_path+'/complex_words_test_unlabeled.txt'\n",
        "all_complex(train_file_name)\n",
        "all_complex(dev_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxnVb5Wu5cNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_length_threshold(train_file_name,dev_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeES8WCeDGWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counts = load_ngram_counts(data_path+ngram_path)\n",
        "avg = sum([val for val in counts.values()])/len(counts)\n",
        "avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjyLRRwrEnvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counts['the']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IygRz_AByyCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ngram_path = '/ngram_counts.txt.gz'\n",
        "\n",
        "word_frequency_threshold(train_file_name,dev_file_name,counts)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJw4nGI3AGKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKRLgZY1OhE9",
        "colab_type": "text"
      },
      "source": [
        "Q 1.3 Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwG-EYuOOmUP",
        "colab_type": "text"
      },
      "source": [
        "1.3.1 Naive Bayes classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zjEwOHYOkv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "naive_bayes(train_file_name,dev_file_name,counts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SkZbaLOyLMg",
        "colab_type": "text"
      },
      "source": [
        "Q1.4. Ambiguity\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHCb8Kk3c7-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "words,labels = load_file(train_file_name)\n",
        "words_dict = defaultdict(list)\n",
        "bad_words = []\n",
        "for word,label in zip(words,labels):\n",
        "  if word in words_dict and label not in words_dict[word]:\n",
        "    bad_words.append(word) \n",
        "  words_dict[word].append(label)\n",
        "\n",
        "print(bad_words)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAowdwJVyQfl",
        "colab_type": "text"
      },
      "source": [
        "Q2. Document Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ifJ-1dzySAl",
        "colab_type": "text"
      },
      "source": [
        "Q2.1. Reuters Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH6T8VKqyT3-",
        "colab_type": "text"
      },
      "source": [
        "2.1.1 Explore how many documents are in the dataset, how many categories, how many documents per categories, provide mean and standard deviation, min and max. (use the pandas library to explore the dataset, use the dataframe.describe() method.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TbWM1S1yXGl",
        "colab_type": "text"
      },
      "source": [
        "**Answer :**\n",
        "-\n",
        "number of topics:  445\n",
        "number of documents:  19716\n",
        "                  \n",
        "count    445.000000\n",
        "mean      89.871910\n",
        "std      644.656909\n",
        "min        1.000000\n",
        "25%        3.000000\n",
        "50%        8.000000\n",
        "75%       37.000000\n",
        "max    12542.000000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gujs0P46FXB2",
        "colab_type": "text"
      },
      "source": [
        " \n",
        "count    445.000000\n",
        "mean      89.871910\n",
        "std      644.656909\n",
        "min        1.000000\n",
        "25%        3.000000\n",
        "50%        8.000000\n",
        "75%       37.000000\n",
        "max    12542.000000\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqYiCOwmFOo5",
        "colab_type": "text"
      },
      "source": [
        "2.1.2 Explore how many characters and words are present in the documents of the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0rw6KSsGLsh",
        "colab_type": "text"
      },
      "source": [
        "*By exploring the dataset we looked at the body of the articles.\n",
        "\n",
        "\n",
        "total words: 2498237\n",
        "total chars: 15089138"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxXnlQ50MPVq",
        "colab_type": "text"
      },
      "source": [
        "2.1.3 Explain informally what are the classifiers that support the \"partial-fit\" method discussed in the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krezwffeMVSo",
        "colab_type": "text"
      },
      "source": [
        "**SGDClassifier**: A linear classifier such as SVM or logistic regression that is trained with SGD algorithm, thus it can support training on mini-batches.\n",
        "\n",
        "**Perceptron**: A linear classifier which updates its weights iteratively for each example (batch of size 1)\n",
        "\n",
        "**NB Multinomial**: The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). \n",
        "\n",
        "**Passive-Agressive**:  The name comes from the fact that we want to reduce loss quickly (aggressive) but we also do not want to overcorrect our weights (passive)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7X976LgQsMl",
        "colab_type": "text"
      },
      "source": [
        "2.1.4 Explain what is the hashing vectorizer used in this tutorial. Why is it important to use this vectorizer to achieve \"streaming classification\"?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fibERQnSQwbY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu48xvDbc058",
        "colab_type": "code",
        "outputId": "ac400530-ec80-48af-c00d-39fc5a0aae7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('conll2002')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2002.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seo6z_nGcujd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import conll2002\n",
        "\n",
        "\n",
        "etr = list(conll2002.iob_sents('esp.train')) # In Spanish\n",
        "eta = list(conll2002.iob_sents('esp.testa')) # In Spanish\n",
        "etb = list(conll2002.iob_sents('esp.testb'))# In Spanish\n",
        "\n",
        "dtr = list(conll2002.iob_sents('ned.train')) # In Dutch\n",
        "dta = list(conll2002.iob_sents('ned.testa')) # In Dutch\n",
        "dtb = list(conll2002.iob_sents('ned.testb')) # In Dutch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxOgN751dKOr",
        "colab_type": "code",
        "outputId": "85ba4cab-7afd-4ebf-ec75-62577c0a64b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "etr[2]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('El', 'DA', 'O'),\n",
              " ('Abogado', 'NC', 'B-PER'),\n",
              " ('General', 'AQ', 'I-PER'),\n",
              " ('del', 'SP', 'I-PER'),\n",
              " ('Estado', 'NC', 'I-PER'),\n",
              " (',', 'Fc', 'O'),\n",
              " ('Daryl', 'VMI', 'B-PER'),\n",
              " ('Williams', 'NC', 'I-PER'),\n",
              " (',', 'Fc', 'O'),\n",
              " ('subrayó', 'VMI', 'O'),\n",
              " ('hoy', 'RG', 'O'),\n",
              " ('la', 'DA', 'O'),\n",
              " ('necesidad', 'NC', 'O'),\n",
              " ('de', 'SP', 'O'),\n",
              " ('tomar', 'VMN', 'O'),\n",
              " ('medidas', 'NC', 'O'),\n",
              " ('para', 'SP', 'O'),\n",
              " ('proteger', 'VMN', 'O'),\n",
              " ('al', 'SP', 'O'),\n",
              " ('sistema', 'NC', 'O'),\n",
              " ('judicial', 'AQ', 'O'),\n",
              " ('australiano', 'AQ', 'O'),\n",
              " ('frente', 'RG', 'O'),\n",
              " ('a', 'SP', 'O'),\n",
              " ('una', 'DI', 'O'),\n",
              " ('página', 'NC', 'O'),\n",
              " ('de', 'SP', 'O'),\n",
              " ('internet', 'NC', 'O'),\n",
              " ('que', 'PR', 'O'),\n",
              " ('imposibilita', 'VMI', 'O'),\n",
              " ('el', 'DA', 'O'),\n",
              " ('cumplimiento', 'NC', 'O'),\n",
              " ('de', 'SP', 'O'),\n",
              " ('los', 'DA', 'O'),\n",
              " ('principios', 'NC', 'O'),\n",
              " ('básicos', 'AQ', 'O'),\n",
              " ('de', 'SP', 'O'),\n",
              " ('la', 'DA', 'O'),\n",
              " ('Ley', 'NC', 'B-MISC'),\n",
              " ('.', 'Fp', 'O')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF12CDbGhaUO",
        "colab_type": "text"
      },
      "source": [
        "3.1.2 Train the model using a logistic regression classifier and experiment with better features - looking at the tags of the previous word, the previous word and the following word (add padding words in the vectorizer)\n",
        "\n",
        "> Indented block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzU5j84VfMrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.parse import urlparse\n",
        "import string\n",
        "puncts = list(string.punctuation) + ['...']\n",
        "\n",
        "def get_ort(w):\n",
        "  if w.isdigit():\n",
        "    return 'number'\n",
        "  elif any([char.isdigit() for char in w]):\n",
        "    return 'contains-digit'\n",
        "  elif w in puncts:\n",
        "    return 'punctuation'\n",
        "  elif ('-' in w):\n",
        "    return 'contains-hyphen'\n",
        "  elif w.istitle():\n",
        "    return 'capitalized'\n",
        "  elif w.isupper():\n",
        "    return 'all-capitals'\n",
        "  elif (urlparse(w).scheme) and (urlparse(w).netloc):\n",
        "    return 'URL'\n",
        "  else:\n",
        "    return 'regular'\n",
        "\n",
        "def word2features(sent, i):\n",
        "    word = sent[i][0]\n",
        "    postag = sent[i][1]\n",
        "\n",
        "\n",
        "    features = {\n",
        "        'word-form': word.lower(),\n",
        "        'ORT': get_ort(word),\n",
        "        'POS': postag,\n",
        "        'suffix3' : word[-3:],\n",
        "        'suffix2' : word[-2:],\n",
        "        'suffix1': word[-1:],\n",
        "        'prefix3': word[:3],\n",
        "        'prefix2': word[:2],\n",
        "        'prefix1': word[:1],\n",
        "    }\n",
        "\n",
        "    if i > 0:\n",
        "        word1 = sent[i-1][0]\n",
        "        postag1 = sent[i-1][1]\n",
        "        features.update({\n",
        "            'prev-word-form' : word1.lower(),\n",
        "            'prev-POS' : postag1,\n",
        "        })\n",
        "    else:\n",
        "        features.update({\n",
        "            'prev-word-form' : 'BOS',\n",
        "            'prev-POS' : '',\n",
        "        })\n",
        "        \n",
        "    if i < len(sent)-1:\n",
        "        word1 = sent[i+1][0]\n",
        "        postag1 = sent[i+1][1]\n",
        "        features.update({\n",
        "            'next-word-form' : word1.lower(),\n",
        "        })\n",
        "    else:\n",
        "        features.update({\n",
        "            'next-word-form': 'EOS'\n",
        "            })\n",
        "                \n",
        "    return features\n",
        "\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return [label for token, postag, label in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [token for token, postag, label in sent]    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw1qKu63iVUd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2135dcc9-6af8-4e8e-d443-fe4112b38ee5"
      },
      "source": [
        "expected = ['number',\n",
        " 'contains-digit',\n",
        " 'punctuation',\n",
        " 'contains-hyphen',\n",
        " 'all-capitals',\n",
        " 'capitalized',\n",
        " 'URL',\n",
        " 'regular']\n",
        "\n",
        "expected == [get_ort(w) for w in ['123','w1',',','hi-hi','SDA','Hello','http://www.google.com','just_word']]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDkf1Zk-oJx5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "c30c71d1-b92e-4048-8082-d6e8b6f03c8f"
      },
      "source": [
        "sen_triplets = etr[100]\n",
        "print(' '.join([triplet[0] for triplet in sen_triplets]))\n",
        "print('-----------------')\n",
        "\n",
        "word2features(sen_triplets,0)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imagínense ustedes que entre aquellos españoles , que fueron quienes llevaron a Europa esos dones americanos , se hubiera impuesto la patriotería gastronómica : patatas y tomates se hubieran quedado en curiosidades botánicas .\n",
            "-----------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ORT': 'capitalized',\n",
              " 'POS': 'VMM',\n",
              " 'next-word-form': 'ustedes',\n",
              " 'prefix1': 'I',\n",
              " 'prefix2': 'Im',\n",
              " 'prefix3': 'Ima',\n",
              " 'prev-POS': '',\n",
              " 'prev-word-form': 'BOS',\n",
              " 'suffix1': 'e',\n",
              " 'suffix2': 'se',\n",
              " 'suffix3': 'nse',\n",
              " 'word-form': 'imagínense'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00gwOdXCdj2S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "6ab723f4-f5e5-4ca0-a6c6-c465165da095"
      },
      "source": [
        "from functools import reduce\n",
        "\n",
        "flatten = lambda l: reduce(lambda x,y: x+y,l)\n",
        "\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "v = DictVectorizer(sparse=True)\n",
        "# >>> D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
        "# >>> X = v.fit_transform(D)\n",
        "\n",
        "\n",
        "X_train_esp = [sent2features(s) for s in etr]\n",
        "y_train_esp = [sent2labels(s) for s in etr]\n",
        "\n",
        "X_train_esp = [sent2features(s) for s in eta]\n",
        "y_valid_esp = [sent2labels(s) for s in eta]\n",
        "\n",
        "X_test_esp = [sent2features(s) for s in etb]\n",
        "Y_test_esp = [sent2labels(s) for s in etb]\n",
        "\n",
        "X_train_dutch = [sent2features(s) for s in etr]\n",
        "y_train_dutch = [sent2labels(s) for s in etr]\n",
        "\n",
        "X_valid_dutch = [sent2features(s) for s in eta]\n",
        "y_valid_dutch = [sent2labels(s) for s in eta]\n",
        "\n",
        "X_test_dutch = [sent2features(s) for s in etb]\n",
        "Y_test_dutch = [sent2labels(s) for s in etb]\n",
        "\n",
        "for x in [X_train_esp,X_train_esp,X_test_esp,X_train_dutch,X_valid_dutch,X_test_dutch]:\n",
        "  flatten(x)\n",
        "\n",
        "#Not good! needs flatten in place.\n",
        "\n",
        "\n",
        "sen_triplets = etr[100]\n",
        "print('Sentence {}:'.format(' '.join([triplet[0] for triplet in sen_triplets])))\n",
        "print('The first word {} feature vector: '.format(sen_triplets[0]))\n",
        "print('-------------------')\n",
        "\n",
        "X_train_esp[100][0]\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Sentence Imagínense ustedes que entre aquellos españoles , que fueron quienes llevaron a Europa esos dones americanos , se hubiera impuesto la patriotería gastronómica : patatas y tomates se hubieran quedado en curiosidades botánicas .:\n",
            "The first word ('Imagínense', 'VMM', 'O') feature vector: \n",
            "-------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ORT': 'capitalized',\n",
              " 'POS': 'VMM',\n",
              " 'next-word-form': 'ustedes',\n",
              " 'prefix1': 'I',\n",
              " 'prefix2': 'Im',\n",
              " 'prefix3': 'Ima',\n",
              " 'prev-POS': '',\n",
              " 'prev-word-form': 'BOS',\n",
              " 'suffix1': 'e',\n",
              " 'suffix2': 'se',\n",
              " 'suffix3': 'nse',\n",
              " 'word-form': 'imagínense'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwdKuLMRoOhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVeYFErYgktw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}