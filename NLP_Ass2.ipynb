{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Ass2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP4iIE0CEzKVo63GzUmA8Um",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NitCoh/NLPAss2/blob/master/NLP_Ass2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9i7dduqdq0s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e06d367-cdca-4552-b40d-1e925d3b598f"
      },
      "source": [
        "print('ass2')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ass2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFXFasvIoOwl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "1e7a3c3c-bd35-4dc0-cdb8-e32a5ffaf8f4"
      },
      "source": [
        "!wget 'https://www.cs.bgu.ac.il/~elhadad/nlp20/hw2_1_data.tar.gz'\n",
        "!tar -xvzf hw2_1_data.tar.gz\n",
        "!mv data ../"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-10 08:54:15--  https://www.cs.bgu.ac.il/~elhadad/nlp20/hw2_1_data.tar.gz\n",
            "Resolving www.cs.bgu.ac.il (www.cs.bgu.ac.il)... 132.72.41.239\n",
            "Connecting to www.cs.bgu.ac.il (www.cs.bgu.ac.il)|132.72.41.239|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 435266 (425K) [application/x-gzip]\n",
            "Saving to: ‘hw2_1_data.tar.gz.5’\n",
            "\n",
            "hw2_1_data.tar.gz.5 100%[===================>] 425.06K   708KB/s    in 0.6s    \n",
            "\n",
            "2020-01-10 08:54:16 (708 KB/s) - ‘hw2_1_data.tar.gz.5’ saved [435266/435266]\n",
            "\n",
            "data/\n",
            "data/complex_words_test_unlabeled.txt\n",
            "data/.DS_Store\n",
            "data/complex_words_development.txt\n",
            "data/complex_words_training.txt\n",
            "mv: cannot move 'data' to '../data': Directory not empty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUyxvJjQeKLE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "c8e01fbe-735e-436b-a5cf-5ecdd182c7c1"
      },
      "source": [
        "#############################################################\n",
        "## ASSIGNMENT 2_1 CODE SKELETON\n",
        "#############################################################\n",
        "\n",
        "from collections import defaultdict\n",
        "import gzip\n",
        "\n",
        "#### Q1.1 Evaluation Metrics ####\n",
        "\n",
        "## Input: y_pred, a list of length n with the predicted labels,\n",
        "## y_true, a list of length n with the true labels\n",
        "\n",
        "## Calculates the precision of the predicted labels\n",
        "def get_precision(y_pred, y_true):\n",
        "    ## YOUR CODE HERE...\n",
        "\n",
        "    return precision\n",
        "    \n",
        "## Calculates the recall of the predicted labels\n",
        "def get_recall(y_pred, y_true):\n",
        "    ## YOUR CODE HERE...\n",
        "\n",
        "    return recall\n",
        "\n",
        "## Calculates the f-score of the predicted labels\n",
        "def get_fscore(y_pred, y_true):\n",
        "    ## YOUR CODE HERE...\n",
        "\n",
        "    return fscore\n",
        "\n",
        "#### 2. Complex Word Identification ####\n",
        "\n",
        "## Loads in the words and labels of one of the datasets\n",
        "def load_file(data_file):\n",
        "    words = []\n",
        "    labels = []   \n",
        "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
        "        i = 0\n",
        "        for line in f:\n",
        "            if i > 0:\n",
        "                line_split = line[:-1].split(\"\\t\")\n",
        "                words.append(line_split[0].lower())\n",
        "                labels.append(int(line_split[1]))\n",
        "            i += 1\n",
        "    return words, labels\n",
        "\n",
        "### 1.2.1: A very simple baseline\n",
        "\n",
        "## Labels every word complex\n",
        "def all_complex(data_file):\n",
        "    ## YOUR CODE HERE...\n",
        "    performance = [precision, recall, fscore]\n",
        "    return performance\n",
        "\n",
        "\n",
        "### 1.2.2: Word length thresholding\n",
        "\n",
        "## Finds the best length threshold by f-score, and uses this threshold to\n",
        "## classify the training and development set\n",
        "def word_length_threshold(training_file, development_file):\n",
        "    ## YOUR CODE HERE\n",
        "    training_performance = [tprecision, trecall, tfscore]\n",
        "    development_performance = [dprecision, drecall, dfscore]\n",
        "    return training_performance, development_performance\n",
        "\n",
        "### 1.2.3: Word frequency thresholding\n",
        "\n",
        "## Loads Google NGram counts\n",
        "def load_ngram_counts(ngram_counts_file): \n",
        "   counts = defaultdict(int) \n",
        "   with gzip.open(ngram_counts_file, 'rt', errors='ignore') as f: \n",
        "       for line in f:\n",
        "           token, count = line.strip().split('\\t') \n",
        "           if token[0].islower(): \n",
        "               counts[token] = int(count) \n",
        "   return counts\n",
        "\n",
        "# Finds the best frequency threshold by f-score, and uses this threshold to\n",
        "## classify the training and development set\n",
        "def word_frequency_threshold(training_file, development_file, counts):\n",
        "    ## YOUR CODE HERE\n",
        "    training_performance = [tprecision, trecall, tfscore]\n",
        "    development_performance = [dprecision, drecall, dfscore]\n",
        "    return training_performance, development_performance\n",
        "\n",
        "### 1.3.1: Naive Bayes\n",
        "        \n",
        "## Trains a Naive Bayes classifier using length and frequency features\n",
        "def naive_bayes(training_file, development_file, counts):\n",
        "    ## YOUR CODE HERE\n",
        "    training_performance = [tprecision, trecall, tfscore]\n",
        "    development_performance = [dprecision, drecall, dfscore]\n",
        "    return training_performance, development_performance\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    training_file = \"../data/complex_words_training.txt\"\n",
        "    development_file = \"../data/complex_words_development.txt\"\n",
        "    test_file = \"../data/complex_words_test_unlabeled.txt\"\n",
        "    train_data = load_file(training_file)\n",
        "    "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e042cc4e1f99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mdevelopment_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../data/complex_words_development.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mtest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../data/complex_words_test_unlabeled.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-e042cc4e1f99>\u001b[0m in \u001b[0;36mload_file\u001b[0;34m(data_file)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/complex_words_training.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3xNjASeoMSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}